{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Homework 4: Extending the GLM to include the following:\n",
    "1. Calculate t-tests for each $beta$\n",
    "2. Calculate an F-statistic, df, and P-value for the overall model $R^2$, comparing the full model to an intercept-only model. \n",
    "3. Add the ability to perform t-tests for a set of contrasts.\n",
    "4. Make your script into a function that takes in a design matrix $X$, data $y$, and optional contrast matrix $C$. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65af94ec80c52dd9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pulling in functions from last assignment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e92d3c6fe4d1cc8a"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T19:32:44.212244Z",
     "start_time": "2023-09-27T19:32:42.913032Z"
    }
   },
   "id": "f1b04a4bc9f21be1"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# estimate the beta coefficients\n",
    "def get_beta_estimates(X, y):\n",
    "    betas = np.linalg.inv(X.T @ X) @ np.dot(X.T, y)\n",
    "    return betas"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T19:29:09.226846Z",
     "start_time": "2023-09-27T19:29:09.214674Z"
    }
   },
   "id": "86dc5ccb33aa64ed"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# estimate the covariance of beta hat\n",
    "def cov_beta_hat(X, y, B):\n",
    "    S = (y - (X @ B)).T @ (y - (X @ B))\n",
    "    n = X.shape[0]\n",
    "    return S/(n-1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T19:29:09.366035Z",
     "start_time": "2023-09-27T19:29:09.355965Z"
    }
   },
   "id": "4ebee8f3396b75ff"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# estimate the sum of squared errors\n",
    "def SS_tot(y):\n",
    "    return np.dot((y - np.mean(y)).T, (y - np.mean(y)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T19:29:09.533280Z",
     "start_time": "2023-09-27T19:29:09.524472Z"
    }
   },
   "id": "6e3f2a35881e3f78"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# estimate the sum of squared residuals\n",
    "def SS_res(X, y, B):\n",
    "    e = y - np.dot(X, B)\n",
    "    return np.dot(e.T, e)\n",
    "# divide this output by n-p to get the variance of the residuals"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T19:29:09.700153Z",
     "start_time": "2023-09-27T19:29:09.687065Z"
    }
   },
   "id": "cf57cb8e7f9f6d32"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# estimate the R squared\n",
    "def R_squared(X, y, B):\n",
    "    return (SS_tot(y) - SS_res(X, y, B))/SS_tot(y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T19:29:09.849064Z",
     "start_time": "2023-09-27T19:29:09.833263Z"
    }
   },
   "id": "65da185633d7b74a"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# put it all together\n",
    "def GLM(X, y):\n",
    "    B = get_beta_estimates(X, y)\n",
    "    S = SS_res(X, y, B)\n",
    "    n = X.shape[0]\n",
    "    cov_beta = cov_beta_hat(X, y, B)\n",
    "    R2 = R_squared(X, y, B)\n",
    "    return B, S, cov_beta, R2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T19:29:09.991347Z",
     "start_time": "2023-09-27T19:29:09.979001Z"
    }
   },
   "id": "831f3d0bbc388d45"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0  id     1     2  group  bpi_intensity  gender  is_patient  \\\n0           0  12  2.50  2.75    3.0           2.50     1.0           1   \n1           2  14  2.50  2.00    3.0           2.50     2.0           1   \n2           4  15  2.25  2.75    3.0           2.25     1.0           1   \n3           6  18  4.50  2.25    2.0           4.50     1.0           1   \n4           8  23  2.50  2.25    2.0           2.50     2.0           1   \n\n   pain_diff  \n0       0.25  \n1      -0.50  \n2       0.50  \n3      -2.25  \n4      -0.25  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>id</th>\n      <th>1</th>\n      <th>2</th>\n      <th>group</th>\n      <th>bpi_intensity</th>\n      <th>gender</th>\n      <th>is_patient</th>\n      <th>pain_diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>12</td>\n      <td>2.50</td>\n      <td>2.75</td>\n      <td>3.0</td>\n      <td>2.50</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>14</td>\n      <td>2.50</td>\n      <td>2.00</td>\n      <td>3.0</td>\n      <td>2.50</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>-0.50</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>15</td>\n      <td>2.25</td>\n      <td>2.75</td>\n      <td>3.0</td>\n      <td>2.25</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>18</td>\n      <td>4.50</td>\n      <td>2.25</td>\n      <td>2.0</td>\n      <td>4.50</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>-2.25</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8</td>\n      <td>23</td>\n      <td>2.50</td>\n      <td>2.25</td>\n      <td>2.0</td>\n      <td>2.50</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>-0.25</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backpain = pd.read_csv('../week2/backpain.csv')\n",
    "# subset to variables of interest\n",
    "var_labels = ['id', 'group', 'bpi_intensity', 'gender', 'is_patient']\n",
    "X = backpain[var_labels].to_numpy()\n",
    "y = backpain['pain_diff'].to_numpy()\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T20:14:02.532846Z",
     "start_time": "2023-09-27T20:14:02.503001Z"
    }
   },
   "id": "15241664fc0acc06"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Calculate t-tests for each $\\beta$\n",
    "\n",
    "The t-statistic for testing $\\beta$ = 0 is given by:\n",
    "$t_j = \\frac{\\hat{\\beta}_j}{\\hat{\\sigma}\\sqrt{(X^T X)^{-1}_{jj}}}$\n",
    "\n",
    "Within that function, I see that I need to calculate the following:\n",
    "1. $\\hat{\\beta}_j$ - the estimate of the beta coefficient for the jth predictor\n",
    "2. $\\hat{\\sigma}$ - the estimated standard deviation of the residuals\n",
    "3. $(X^T X)^{-1}_{jj}$ - the jth diagonal element of the inverse of the product of the design matrix and its transpose. This essentially means I am calculating the covariance matrix of the design matrix, inverting it, and then taking the jth diagonal element of that matrix (where j is the index of the predictor in the design matrix).\n",
    "        - Inside this function term, I see $X^T X$. This is the product of the design matrix and its transpose, which I know to be the covariance matrix of the design matrix. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1b5d5de64156c48"
  },
  {
   "cell_type": "markdown",
   "source": [
    "My understanding is that the t-statistic is the ratio of the estimated coefficient to the estimated standard deviation of the residuals. The estimated standard deviation of the residuals is the square root of the sum of squared residuals divided by the degrees of freedom. The degrees of freedom is the number of observations minus the number of predictors.\n",
    "\n",
    "So, reducing the the standard deviation of the residuals would mean that I would see less spread of the data points around the regression line. Less spread means that my beta can be smaller but still be significant. In the same vein, if I have a large spread around that line, then my beta would have to be larger to be significant.\n",
    "\n",
    "Also, adding more data points to an otherwise very noisy cloud of data points surrounding the regression line would reduce the standard deviation of the residuals. This is because the sum of squared residuals would be larger, but the degrees of freedom would also be larger. So, the ratio of the two would be smaller."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63eee7135f2d880c"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "# make a function for the estimated variance of the residuals\n",
    "def sigma_hat(X, y, B):\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    return SS_res(X, y, B)/(n-p)\n",
    "\n",
    "def t_stat(X, y, B):\n",
    "    n = X.shape[0] # number of observations\n",
    "    p = X.shape[1] # number of predictors\n",
    "    # get sigma_hat (estimated variance of residuals)\n",
    "    t = {}\n",
    "    for j, varname in enumerate(var_labels):\n",
    "        t[varname] = B[j] / (sigma_hat(X,y,B)**2 * np.sqrt(np.linalg.inv(X.T @ X)[j,j]))\n",
    "        # reminder - try replacing the * with @ to see if that changes anything.\n",
    "        \n",
    "    return t\n",
    "        \n",
    "    \"\"\"\n",
    "    This function is an algorithm. For each beta, it finds the index of the beta in the beta vector. This is provided \n",
    "    by the j term of the enumerate function in the loop. The name of the variable, `varname`, is also provided as the second\n",
    "    value of each iteration of the loop. The variable `t` is a dictionary. The key is the variable name, and the value is the\n",
    "    t-statistic for that variable (these key-value pairs are populated on each successive iteration of the loop. \n",
    "    \n",
    "    On each iteration, the t-statistic is calculated by dividing the beta estimate Bj by the product of the estimated standard \n",
    "    deviation of the residuals. \n",
    "    \n",
    "    The function returns the dictionary of t-statistics.\n",
    "    \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T20:14:06.820297Z",
     "start_time": "2023-09-27T20:14:06.812302Z"
    }
   },
   "id": "1f62962f3d51071d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below I'm gonna first get the betas and other values from the GLM function I wrote in the last assignment. Then I'm gonna use those values to calculate the t-statistic for each beta."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4470b89e3d6da68a"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id beta = : 0.0003140003130136992\n",
      "group beta = : 1.0560804398642403\n",
      "bpi_intensity beta = : 0.009906787275012041\n",
      "gender beta = : -0.1855060030488982\n",
      "is_patient beta = : -3.429480968417778\n",
      "\n",
      "Sum of squared residuals:  215.02230361367157\n",
      "Covariance of beta hat:  1.6046440568184446\n",
      "R squared:  0.33238629060675456\n"
     ]
    }
   ],
   "source": [
    "B, S, cov_beta, R2 = GLM(X, y)\n",
    "for i, var in enumerate(var_labels):\n",
    "    print(f'{var} beta = : {B[i]}')\n",
    "print('\\nSum of squared residuals: ', S)\n",
    "print('Covariance of beta hat: ', cov_beta)\n",
    "print('R squared: ', R2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T20:14:07.881844Z",
     "start_time": "2023-09-27T20:14:07.874579Z"
    }
   },
   "id": "11e41f5004ede007"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now I'm gonna use the t_stat function I wrote above to calculate the t-statistic for each beta."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c80870a6d0c4e196"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-statistics:  {'id': 0.5445476669625823, 'group': 3.624534678997911, 'bpi_intensity': 0.06343298420211381, 'gender': -0.3926125821888913, 'is_patient': -3.00887657599113}\n"
     ]
    }
   ],
   "source": [
    "tvals = t_stat(X, y, B)\n",
    "print('T-statistics: ', tvals)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T20:14:17.716961Z",
     "start_time": "2023-09-27T20:14:17.703189Z"
    }
   },
   "id": "c4d6c41a8cd14246"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Yay! Now, for sanity's sake, I'm gonna compare my t-statistics to the ones from the statsmodels package."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "646e49ad54ea4180"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.332\n",
      "Model:                            OLS   Adj. R-squared:                  0.312\n",
      "Method:                 Least Squares   F-statistic:                     16.18\n",
      "Date:                Wed, 27 Sep 2023   Prob (F-statistic):           8.88e-11\n",
      "Time:                        16:14:23   Log-Likelihood:                -222.98\n",
      "No. Observations:                 135   AIC:                             456.0\n",
      "Df Residuals:                     130   BIC:                             470.5\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             0.0003      0.000      1.158      0.249      -0.000       0.001\n",
      "x2             1.0561      0.137      7.710      0.000       0.785       1.327\n",
      "x3             0.0099      0.073      0.135      0.893      -0.135       0.155\n",
      "x4            -0.1855      0.222     -0.835      0.405      -0.625       0.254\n",
      "const         -3.4295      0.536     -6.401      0.000      -4.490      -2.369\n",
      "==============================================================================\n",
      "Omnibus:                        8.858   Durbin-Watson:                   2.009\n",
      "Prob(Omnibus):                  0.012   Jarque-Bera (JB):                8.995\n",
      "Skew:                          -0.517   Prob(JB):                       0.0111\n",
      "Kurtosis:                       3.728   Cond. No.                     3.90e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 3.9e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "[ 1.1583669   7.71014413  0.13493524 -0.83516916 -6.40051044]\n"
     ]
    }
   ],
   "source": [
    "X_const = sm.add_constant(X)\n",
    "model = sm.OLS(y, X_const)\n",
    "results = model.fit()\n",
    "print(results.summary()) \n",
    "print(results.tvalues)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T20:14:23.403257Z",
     "start_time": "2023-09-27T20:14:23.377616Z"
    }
   },
   "id": "8df89cff2100403"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Whoops, looks like I made a mistake. My hand-calculated t-values are much smaller than the ones from the statsmodels package. However, they are all smaller by the same proportion. So I'm close, but clearly something is wrong with how I calculated them. \n",
    "\n",
    "Again, here's the formula for the t-statistic:\n",
    "$t_j = \\frac{\\hat{\\beta}_j}{\\hat{\\sigma}\\sqrt{(X^T X)^{-1}_{jj}}}$\n",
    "\n",
    "And here's my code:\n",
    "```\n",
    "def t_stat(X, y, B):\n",
    "    n = X.shape[0] # number of observations\n",
    "    p = X.shape[1] # number of predictors\n",
    "    # get sigma_hat (estimated variance of residuals)\n",
    "    t = {}\n",
    "    for j, varname in enumerate(var_labels):\n",
    "        t[varname] = B[j] / (sigma_hat(X,y,B)**2 * np.sqrt(np.linalg.inv(X.T @ X)[j,j]))\n",
    "        # reminder - try replacing the * with @ to see if that changes anything.\n",
    "        \n",
    "    return t\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2ed8e5a996f0a97"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The problem is that I'm squaring the estimated standard deviation of the residuals in `sigma_hat(X,y,B)**2`, where I should be taking the square root! This is because my function for $\\hat\\sigma$ actually returns the sum of squared residuals divided by the degrees of freedom. So, I need to take the square root of that value to get the estimated standard deviation of the residuals (perhaps I need to rename the function). "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f643c57463c469"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "def t_stat(X, y, B):\n",
    "    n = X.shape[0] # number of observations\n",
    "    p = X.shape[1] # number of predictors\n",
    "    # get sigma_hat (estimated variance of residuals)\n",
    "    t = {}\n",
    "    for j, varname in enumerate(var_labels):\n",
    "        ### NOW TAKE SQRT ###\n",
    "        t[varname] = B[j] / (np.sqrt(sigma_hat(X,y,B)) * np.sqrt(np.linalg.inv(X.T @ X)[j,j]))\n",
    "        \n",
    "    return t"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T20:27:29.173820Z",
     "start_time": "2023-09-27T20:27:29.158766Z"
    }
   },
   "id": "c19a52345ff726e7"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-statistics:  {'id': 1.1583668987127718, 'group': 7.710144125318964, 'bpi_intensity': 0.13493523826142437, 'gender': -0.8351691629908486, 'is_patient': -6.40051044086071}\n"
     ]
    }
   ],
   "source": [
    "tvals = t_stat(X, y, B)\n",
    "print('T-statistics: ', tvals)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T20:27:29.768287Z",
     "start_time": "2023-09-27T20:27:29.755984Z"
    }
   },
   "id": "f5f81951fc8672c2"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "# saving these to put in the table later\n",
    "tstats = list(tvals.values())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T20:36:47.513899Z",
     "start_time": "2023-09-27T20:36:47.502909Z"
    }
   },
   "id": "b1cfe0b1ee8582b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Great - now I have the right t-values. Next I'm going to calculate the p-values for each t-statistic.\n",
    "\n",
    "From the book:\n",
    "The P-value comes from a t-distribution, which is a Normal (Gaussian) distribution adjusted for the fact that there are not really $n$ independent observations, and thus not really $n$ error df, if we have estimated some parameters ($p$ parameters, to be precise) and removed them when estimating the residual error. The t-statistic, $t_j$, can be compared to a t-distribution with $n - p$ degrees of freedom to determine the significance of the $j$-th predictor.\n",
    "\n",
    "So to get p-values I really need to generate a null <em>distribution</em>, rather than a singular value. I think the best way to do this in Python is to use pre-made functionality to generate a cumulative distribution function (CDF) for the t-distribution, and then use that to calculate the p-value for each t-statistic. I'll use the `stats` module within the scipy package, and the `t.sf` function to calculate the p-value for each t-statistic. I'll also assume we want a two-tailed distribution for now, so I'll multiply the p-value by 2.\n",
    "\n",
    "The `t.sf` function takes two arguments: the t-statistic and the degrees of freedom. It represents the survival function, which is the complement of the cumulative distribution function (CDF). "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f69a4489d005bb0a"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "from scipy import stats"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T20:42:48.389670Z",
     "start_time": "2023-09-27T20:42:48.352090Z"
    }
   },
   "id": "e66f589fbaff21e4"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "# get the degrees of freedom\n",
    "n = X.shape[0]\n",
    "p = X.shape[1]\n",
    "df = n - p"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T20:43:00.549253Z",
     "start_time": "2023-09-27T20:43:00.518673Z"
    }
   },
   "id": "b66245a3cd91cbc1"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-values:  {'id': 0.24883791360348023, 'group': 2.8747951957998716e-12, 'bpi_intensity': 0.8928717759108604, 'gender': 0.40515472362610394, 'is_patient': 2.5797920691951886e-09}\n"
     ]
    }
   ],
   "source": [
    "# get the p-values\n",
    "pvals = {}\n",
    "for varname, tstat in tvals.items():\n",
    "    pvals[varname] = stats.t.sf(np.abs(tstat), df)*2\n",
    "    \n",
    "# saving these for later\n",
    "pvalues = list(pvals.values())\n",
    "print('P-values: ', pvals)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T20:47:57.571068Z",
     "start_time": "2023-09-27T20:47:57.554730Z"
    }
   },
   "id": "50e5c724e4acd918"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now I'll integrate all this into a new and improved GLM function."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72f72921f441fc3f"
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "def fancy_GLM(X, y, predictor_names):\n",
    "    B, S, cov_beta, R2 = GLM(X, y)\n",
    "    tvals = t_stat(X, y, B)\n",
    "    pvals = {}\n",
    "    for varname, tstat in tvals.items():\n",
    "        pvals[varname] = stats.t.sf(np.abs(tstat), df)*2\n",
    "        \n",
    "    output = pd.DataFrame({'Predictor': predictor_names, 'Beta': B, 'T-statistic': tstats, 'P-value (two-tailed)': pvalues})\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T20:51:39.188166Z",
     "start_time": "2023-09-27T20:51:39.169811Z"
    }
   },
   "id": "a9b498acfaedecf3"
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "       Predictor      Beta  T-statistic  P-value (two-tailed)\n0             id  0.000314     1.158367          2.488379e-01\n1          group  1.056080     7.710144          2.874795e-12\n2  bpi_intensity  0.009907     0.134935          8.928718e-01\n3         gender -0.185506    -0.835169          4.051547e-01\n4     is_patient -3.429481    -6.400510          2.579792e-09",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Predictor</th>\n      <th>Beta</th>\n      <th>T-statistic</th>\n      <th>P-value (two-tailed)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id</td>\n      <td>0.000314</td>\n      <td>1.158367</td>\n      <td>2.488379e-01</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>group</td>\n      <td>1.056080</td>\n      <td>7.710144</td>\n      <td>2.874795e-12</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>bpi_intensity</td>\n      <td>0.009907</td>\n      <td>0.134935</td>\n      <td>8.928718e-01</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>gender</td>\n      <td>-0.185506</td>\n      <td>-0.835169</td>\n      <td>4.051547e-01</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>is_patient</td>\n      <td>-3.429481</td>\n      <td>-6.400510</td>\n      <td>2.579792e-09</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model_results = fancy_GLM(X, y, var_labels)\n",
    "new_model_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T20:51:41.342988Z",
     "start_time": "2023-09-27T20:51:41.327073Z"
    }
   },
   "id": "56eade78bfc496ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Calculate an F-statistic, df, and P-value for the overall model $R^2$, comparing the full model to an intercept-only model.\n",
    "\n",
    "From the book:\n",
    "\n",
    "The F-test is used to compare the fits of different models. Specifically, it is used to test the hypothesis that a set of predictors has no effect on the response variable. \n",
    "\n",
    "Given:\n",
    "\n",
    " $SSR_full$ = Sum of Squared Residuals for the full model\n",
    "\n",
    " $SSR_reduced$ = Sum of Squared Residuals for a reduced model\n",
    "\n",
    " $p$ = Number of predictors in the full model\n",
    "\n",
    " $q$ = Number of predictors in the reduced model (with)\n",
    "\n",
    " $n$ = Total number of observations\n",
    "\n",
    "The F-statistic is caluclated as:\n",
    "\n",
    "\n",
    "$$F = \\frac{(SSR_{\\text{reduced}} - SSR_{\\text{full}}) / (p - q)}{SSR_{\\text{full}} / (n - p)}$$\n",
    "\n",
    "The difference between $SSR_reduced$ and $SSR_full$ is that $SSR_reduced$ is the sum of squared residuals for the reduced model, which is the model with fewer predictors. $SSR_full$ is the sum of squared residuals for the full model, which is the model with more predictors.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f75be62303c53cab"
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of squared residuals for intercept-only model:  322.07592592592596\n"
     ]
    }
   ],
   "source": [
    "# calculate the sum of squared residuals for the intercept-only model (this just means like... the mean of the outcome variable)\n",
    "X_intercept = np.ones((X.shape[0], 1)) # this is a column of ones - statsmodels can do this with sm.add_constant(X)\n",
    "B_intercept, S_intercept, cov_beta_intercept, R2_intercept = GLM(X_intercept, y) \n",
    "print('Sum of squared residuals for intercept-only model: ', S_intercept) # this is SSreduced"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T21:19:14.352755Z",
     "start_time": "2023-09-27T21:19:14.338298Z"
    }
   },
   "id": "56ff5c7d5bd78b4f"
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance:  107.05362231225439\n"
     ]
    }
   ],
   "source": [
    "# calculate the explained variance\n",
    "explained_variance = S_intercept - S\n",
    "print('Explained variance: ', explained_variance) # this is SSRreduced - SSRfull"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-27T21:19:42.915807Z"
    }
   },
   "id": "26c9cfd9b1de5bb8"
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (135,1) and (5,) not aligned: 1 (dim 1) != 5 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[108], line 10\u001B[0m\n\u001B[1;32m      7\u001B[0m     F \u001B[38;5;241m=\u001B[39m ((SSR_reduced \u001B[38;5;241m-\u001B[39m SSR_full)\u001B[38;5;241m/\u001B[39m(p\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m/\u001B[39m(SSR_full\u001B[38;5;241m/\u001B[39m(n\u001B[38;5;241m-\u001B[39mp))\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\n\u001B[0;32m---> 10\u001B[0m F \u001B[38;5;241m=\u001B[39m get_F(X, y, B)\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mF-statistic: \u001B[39m\u001B[38;5;124m'\u001B[39m, F)\n",
      "Cell \u001B[0;32mIn[108], line 6\u001B[0m, in \u001B[0;36mget_F\u001B[0;34m(X, y, B)\u001B[0m\n\u001B[1;32m      4\u001B[0m q \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;66;03m# number of predictors in the reduced model (only 1 here bc intercept-only model)\u001B[39;00m\n\u001B[1;32m      5\u001B[0m SSR_full \u001B[38;5;241m=\u001B[39m SS_res(X, y, B) \u001B[38;5;66;03m# ss for full model\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m SSR_reduced \u001B[38;5;241m=\u001B[39m SS_res(np\u001B[38;5;241m.\u001B[39mones((n, \u001B[38;5;241m1\u001B[39m)), y, B) \u001B[38;5;66;03m# ss for intercept-only model (reduce model)\u001B[39;00m\n\u001B[1;32m      7\u001B[0m F \u001B[38;5;241m=\u001B[39m ((SSR_reduced \u001B[38;5;241m-\u001B[39m SSR_full)\u001B[38;5;241m/\u001B[39m(p\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m/\u001B[39m(SSR_full\u001B[38;5;241m/\u001B[39m(n\u001B[38;5;241m-\u001B[39mp))\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F\n",
      "Cell \u001B[0;32mIn[48], line 3\u001B[0m, in \u001B[0;36mSS_res\u001B[0;34m(X, y, B)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mSS_res\u001B[39m(X, y, B):\n\u001B[0;32m----> 3\u001B[0m     e \u001B[38;5;241m=\u001B[39m y \u001B[38;5;241m-\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(X, B)\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mdot(e\u001B[38;5;241m.\u001B[39mT, e)\n",
      "\u001B[0;31mValueError\u001B[0m: shapes (135,1) and (5,) not aligned: 1 (dim 1) != 5 (dim 0)"
     ]
    }
   ],
   "source": [
    "def get_F(X, y, B):\n",
    "    n = X.shape[0] # number of observations\n",
    "    p = X.shape[1] # number of predictors in the full model\n",
    "    q = 1 # number of predictors in the reduced model (only 1 here bc intercept-only model)\n",
    "    SSR_full = SS_res(X, y, B) # ss for full model\n",
    "    SSR_reduced = SS_res(np.ones((n, 1)), y, B) # ss for intercept-only model (reduce model)\n",
    "    F = ((SSR_reduced - SSR_full)/(p-1))/(SSR_full/(n-p))\n",
    "    return F\n",
    "\n",
    "F = get_F(X, y, B)\n",
    "print('F-statistic: ', F)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-27T21:23:29.404136Z"
    }
   },
   "id": "c3475f811bbfada1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To get the p-value for the F I'm using `scipy.stats.f.sf`, which is like the function I used for the t-statistic, but for the F-statistic. It takes three arguments: the F-statistic, the degrees of freedom for the numerator, and the degrees of freedom for the denominator. The degrees of freedom for the numerator is the number of predictors in the full model minus the number of predictors in the intercept-only model (in our case, . The degrees of freedom for the denominator is the number of observations minus the number of predictors in the full model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac11f64309a20193"
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "# get the p-value for the F-statistic\n",
    "pval_F = stats.f.sf(F, 1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T21:01:51.594854Z",
     "start_time": "2023-09-27T21:01:51.529661Z"
    }
   },
   "id": "4a3f7f898e3ffd1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8f1332193cec72ad"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
